# -*- coding: utf-8 -*-
"""ML final lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C0-xSTLNug0Xcks5UnUMDtN-Rx8hXaX8
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score, davies_bouldin_score, calinski_harabasz_score
#from sklearn.metrics import normalized_mutual_info_score, fowlkes_mallows_score, homogeneity_completeness_v_measure
from sklearn.metrics.cluster import contingency_matrix
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch, MeanShift
from sklearn.mixture import GaussianMixture
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage

# Sample data
data = pd.read_csv("/content/sample_data/data.csv")

# 1. Handling Missing Values
imputer = SimpleImputer(strategy='mean')
data_imputed = imputer.fit_transform(data)

# 2. Scaling and Normalization
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_imputed)

# normalizer = MinMaxScaler()
# data_normalized = normalizer.fit_transform(data_imputed)

# 3. Handling Categorical Variables
# Assuming data_categorical is a part of your dataset
# data_categorical = pd.DataFrame({'category': ['A', 'B', 'A', 'C']})
# encoder = OneHotEncoder()
# data_encoded = encoder.fit_transform(data_categorical).toarray()

# 4. Dealing with Outliers
z_scores = np.abs(stats.zscore(data_scaled))
data_no_outliers = data_scaled[(z_scores < 3).all(axis=1)]

# 5. Dimensionality Reduction
pca = PCA(n_components=2)
data_reduced = pca.fit_transform(data_no_outliers)

# 6. Feature Selection (removed since we don't have true labels)

# 7. Creating a Similarity Matrix (for Spectral Clustering)
from sklearn.metrics.pairwise import rbf_kernel
similarity_matrix = rbf_kernel(data_no_outliers, gamma=1.0)

# Clustering Algorithms
# K-Means Clustering
kmeans = KMeans(n_clusters=3)
kmeans_labels = kmeans.fit_predict(data_no_outliers)

# Hierarchical Clustering (Agglomerative)
hc = AgglomerativeClustering(n_clusters=3)
hc_labels = hc.fit_predict(data_no_outliers)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(data_no_outliers)

# Gaussian Mixture Models (GMM)
gmm = GaussianMixture(n_components=3)
gmm_labels = gmm.fit_predict(data_no_outliers)

# Spectral Clustering
spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors')
spectral_labels = spectral.fit_predict(data_no_outliers)

# BIRCH
birch = Birch(n_clusters=3)
birch_labels = birch.fit_predict(data_no_outliers)

# Mean Shift
mean_shift = MeanShift()
mean_shift_labels = mean_shift.fit_predict(data_no_outliers)

# Visualization
# Scatter plot for PCA-reduced data
def plot_clusters(data, labels, title):
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=labels, palette='viridis')
    plt.title(title)
    plt.show()

plot_clusters(data_reduced, kmeans_labels, 'K-Means Clustering')
plot_clusters(data_reduced, hc_labels, 'Agglomerative Clustering')
plot_clusters(data_reduced, dbscan_labels, 'DBSCAN Clustering')
plot_clusters(data_reduced, gmm_labels, 'Gaussian Mixture Model Clustering')
plot_clusters(data_reduced, spectral_labels, 'Spectral Clustering')
plot_clusters(data_reduced, birch_labels, 'BIRCH Clustering')
plot_clusters(data_reduced, mean_shift_labels, 'Mean Shift Clustering')

Z = linkage(data_no_outliers, method='ward')

# Plot the dendrogram using the built-in function from SciPy
plt.figure(figsize=(10, 7))
plt.title("Hierarchical Clustering Dendrogram (using built-in function)")
dendrogram(Z, truncate_mode="level", p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

# Dendrogram for Hierarchical Clustering
# def plot_dendrogram(model, **kwargs):
#     counts = np.zeros(model.children_.shape[0])
#     n_samples = len(model.labels_)
#     for i, merge in enumerate(model.children_):
#         current_count = 0
#         for child_idx in merge:
#             if child_idx < n_samples:
#                 current_count += 1
#             else:
#                 current_count += counts[child_idx - n_samples]
#         counts[i] = current_count
#     linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)
#     dendrogram(linkage_matrix, **kwargs)

# model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)
# model = model.fit(data_no_outliers)
# plt.title("Hierarchical Clustering Dendrogram")
# plot_dendrogram(model, truncate_mode="level", p=3)
# plt.xlabel("Number of points in node (or index of point if no parenthesis).")
# plt.show()
# Evaluation Metrics
# Silhouette Score
silhouette_scores = {
    'K-Means': silhouette_score(data_no_outliers, kmeans_labels),
    'Hierarchical Clustering': silhouette_score(data_no_outliers, hc_labels),
    'Gaussian Mixture Models': silhouette_score(data_no_outliers, gmm_labels),
    'Spectral Clustering': silhouette_score(data_no_outliers, spectral_labels),
    'BIRCH': silhouette_score(data_no_outliers, birch_labels),
}

# Davies-Bouldin Index
dbi_scores = {
    'K-Means': davies_bouldin_score(data_no_outliers, kmeans_labels),
    'Hierarchical Clustering': davies_bouldin_score(data_no_outliers, hc_labels),
    'Gaussian Mixture Models': davies_bouldin_score(data_no_outliers, gmm_labels),
    'Spectral Clustering': davies_bouldin_score(data_no_outliers, spectral_labels),
    'BIRCH': davies_bouldin_score(data_no_outliers, birch_labels),
}

# Calinski-Harabasz Index
ch_scores = {
    'K-Means': calinski_harabasz_score(data_no_outliers, kmeans_labels),
    'Hierarchical Clustering': calinski_harabasz_score(data_no_outliers, hc_labels),
    'Gaussian Mixture Models': calinski_harabasz_score(data_no_outliers, gmm_labels),
    'Spectral Clustering': calinski_harabasz_score(data_no_outliers, spectral_labels),
    'BIRCH': calinski_harabasz_score(data_no_outliers, birch_labels),
}

# Sum of Squared Errors (SSE)
sse_scores = {
    'K-Means': kmeans.inertia_,
    'Hierarchical Clustering': None,  # Not applicable for Hierarchical Clustering
    'Gaussian Mixture Models': None,  # Not applicable for Gaussian Mixture Models
    'Spectral Clustering': None,  # Not applicable for Spectral Clustering
    'BIRCH': None,  # Not applicable for BIRCH
}

# Print Evaluation Metrics
print("Silhouette Scores:")
for algorithm, score in silhouette_scores.items():
    print(f"{algorithm}: {score}")

print("\nDavies-Bouldin Index Scores:")
for algorithm, score in dbi_scores.items():
    print(f"{algorithm}: {score}")

print("\nCalinski-Harabasz Index Scores:")
for algorithm, score in ch_scores.items():
    print(f"{algorithm}: {score}")

print("\nSum of Squared Errors (SSE) Scores:")
for algorithm, score in sse_scores.items():
    if score is not None:
        print(f"{algorithm}: {score}")
    else:
        print(f"{algorithm}: Not applicable")
import matplotlib.pyplot as plt

# Silhouette Scores
plt.figure(figsize=(10, 6))
plt.bar(silhouette_scores.keys(), silhouette_scores.values())
plt.xlabel('Clustering Algorithm')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Scores')
plt.show()

# Davies-Bouldin Index Scores
plt.figure(figsize=(10, 6))
plt.bar(dbi_scores.keys(), dbi_scores.values())
plt.xlabel('Clustering Algorithm')
plt.ylabel('Davies-Bouldin Index')
plt.title('Davies-Bouldin Index Scores')
plt.show()

# Calinski-Harabasz Index Scores
plt.figure(figsize=(10, 6))
plt.bar(ch_scores.keys(), ch_scores.values())
plt.xlabel('Clustering Algorithm')
plt.ylabel('Calinski-Harabasz Index')
plt.title('Calinski-Harabasz Index Scores')
plt.show()

# Sum of Squared Errors (SSE) Scores
plt.figure(figsize=(10, 6))
plt.bar([k for k, v in sse_scores.items() if v is not None], [v for v in sse_scores.values() if v is not None])
plt.xlabel('Clustering Algorithm')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.title('Sum of Squared Errors (SSE) Scores')
plt.show()
import seaborn as sns
import pandas as pd

# Create a DataFrame with evaluation metrics
df = pd.DataFrame({
    'Silhouette Score': silhouette_scores.values(),
    'Davies-Bouldin Index': dbi_scores.values(),
    'Calinski-Harabasz Index': ch_scores.values(),
    'Sum of Squared Errors (SSE)': [v for v in sse_scores.values() if v is not None]
}, index=silhouette_scores.keys())

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', square=True)
plt.title('Correlation between Evaluation Metrics')
plt.show()
import matplotlib.pyplot as plt

# Scatter plot of Silhouette Score vs Davies-Bouldin Index
plt.figure(figsize=(10, 6))
plt.scatter(silhouette_scores.values(), dbi_scores.values())
plt.xlabel('Silhouette Score')
plt.ylabel('Davies-Bouldin Index')
plt.title('Silhouette Score vs Davies-Bouldin Index')
plt.show()